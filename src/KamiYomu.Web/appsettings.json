{
  "Kestrel": {
    "Endpoints": {
      "Http": {
        "Url": "http://*:8080"
      }
    }
  },
  "Serilog": {
    "Using": [ "Serilog.Sinks.File", "Serilog.Sinks.Console" ],
    "MinimumLevel": {
      "Default": "Debug",
      "Override": {
        "Microsoft": "Information",
        "System": "Information"
      }
    },
    "WriteTo": [
      {
        "Name": "File",
        "Args": {
          "path": "/logs/log-.txt",
          "rollingInterval": "Day",
          "retainedFileCountLimit": 7,
          "restrictedToMinimumLevel": "Information",
          "outputTemplate": "[{Timestamp:yyyy-MM-dd HH:mm:ss} {Level:u3}] {Message:lj}{NewLine}{Exception}"
        }
      },
      {
        "Name": "Console",
        "Args": {
          "restrictedToMinimumLevel": "Debug",
          "outputTemplate": "[{Timestamp:HH:mm:ss} {Level:u3}] {Message:lj}{NewLine}{Exception}"
        }
      }
    ],
    "Enrich": [ "FromLogContext", "WithMachineName", "WithThreadId" ],
    "Properties": {
      "Application": "KamiYomu"
    }
  },
  "AllowedHosts": "*",
  "ConnectionStrings": {
    "AgentDb": "Filename=/db/main.db;Connection=shared;",
    "ImageDb": "Filename=/db/image.db;Connection=shared;",
    "WorkerDb": "/db/worker.db"
  },
  "SpecialFolders": {
    "LogDir": "/logs",
    "AgentsDir": "/agents",
    "DbDir": "/db",
    "MangaDir": "/manga"
  },
  "UI": {
    "DefaultLanguage": "en"
  },
  "Worker": {
    // List of Hangfire server identifiers available to process jobs. 
    // Each name corresponds to a distinct background worker instance; 
    // add more entries here if you want multiple servers to share or divide queues.
    "ServerAvailableNames": [
      "KamiYomu-background-1"
    ],

    // # Specifies the number of background processing threads Hangfire will spawn.
    // # Increasing this value allows more jobs to run concurrently, but also raises CPU load and memory usage.
    // # Each worker consumes ~80 MB of memory on average while active 
    // # (actual usage may vary depending on the crawler agent implementation and system configuration).
    "WorkerCount": 1,

    // Defines the maximum number of crawler instances allowed to run concurrently for the same source.
    // Typically set to 1 to ensure only a single crawler operates at a time, preventing duplicate work,
    // resource conflicts, and potential rateâ€‘limiting or blocking by the target system.
    // However, this can be adjusted to increase throughput if the source can handle multiple concurrent requests.
    "MaxConcurrentCrawlerInstances": 1,

    // Minimum delay (in milliseconds) between job executions.
    // Helps throttle requests to external services and avoid hitting rate limits (e.g., HTTP 423 "Too Many Requests").
    "MinWaitPeriodInMilliseconds": 3000,

    // Maximum delay (in milliseconds) between job executions.
    // Provides variability in scheduling to reduce the chance of IP blocking or service throttling.
    "MaxWaitPeriodInMilliseconds": 7001,

    // Maximum number of retry attempts for failed jobs before marking them as permanently failed.
    "MaxRetryAttempts": 10,

    // Queue dedicated to downloading individual chapters.
    "DownloadChapterQueues": [
      "download-chapter-queue-1"
    ],

    // Queue dedicated to scheduling manga downloads (manages chapter download jobs).
    "MangaDownloadSchedulerQueues": [
      "manga-download-scheduler-queue-1"
    ],

    // Queue dedicated to discovering new chapters (polling or scraping for updates).
    "DiscoveryNewChapterQueues": [
      "discovery-new-chapter-queue-1"
    ]
  }

}
